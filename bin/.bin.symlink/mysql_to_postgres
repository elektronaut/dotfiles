#!/usr/bin/env ruby

require "bundler/inline"

gemfile do
  source "https://rubygems.org"
  gem "mysql2"
  gem "pg"
  gem "progress_bar"
end

require "pp"
require "yaml"

source_db, target_db = ARGV
target_db ||= source_db

unless source_db
  puts "Usage: #{$0} source_db [target_db]"
  exit
end

class BulkInserter
  attr_reader :db, :table, :columns

  def initialize(db, table, columns)
    @db = db
    @table = table
    @columns = columns
    @queue = []
  end

  def insert(values)
    @queue << values
    flush! if @queue.length > 100
  end

  def flush!
    return if @queue.empty?

    db.exec_params(
      "INSERT INTO \"#{table}\" (#{column_names}) VALUES #{placeholders}",
      @queue.flatten
    )

    @queue = []
  end

  private

  def column_names
    columns.keys.map { |k| "\"#{k}\"" }.join(", ")
  end

  def placeholders
    (1..(@queue.length * columns.length))
      .map { |n| "$#{n}" }
      .each_slice(columns.length)
      .map { |v| "(#{v.join(', ')})" }
      .join(", ")
  end
end

source = Mysql2::Client.new(
  host: "localhost",
  username: ENV["MYSQL_USER"] || "root",
  password: ENV["MYSQL_PASSWORD"] || nil,
  database: source_db,
  encoding: "utf8mb4",
  collation: "utf8mb4_unicode_ci"
)

target = PG.connect(dbname: target_db)

def coerce_value(type, value)
  if type == "boolean"
    value == 1
  else
    if value.is_a?(String) && value.match?("\u0000")
      raise value.inspect
    end
    value
  end
end

tables = target.exec("SELECT table_name
                      FROM information_schema.tables
                      WHERE table_schema = 'public' AND table_name != 'schema_migrations'")
               .each_with_object({}) do |row, hash|
  table_name = row.values_at("table_name").first

  hash[table_name] = target.exec_params(
    "SELECT column_name, data_type FROM information_schema.columns
     WHERE table_schema = 'public' AND table_name = $1",
    [table_name]
  ).each_with_object({}) do |col_row, col_hash|
    col_hash[col_row.values_at("column_name").first] = {
      data_type: col_row.values_at("data_type").first
    }
  end
end

target.exec("BEGIN")

puts "Truncating target tables..."
tables.keys.each do |table|
  next unless source.query("SHOW TABLES LIKE \"#{table}\"").to_a.any?
  target.exec("TRUNCATE \"#{table}\" RESTART IDENTITY CASCADE")
end


tables.each do |table, columns|
  unless source.query("SHOW TABLES LIKE \"#{table}\"").to_a.any?
    puts "Warning: Source table '#{table}' does not exist, skipping..."
    next
  end

  row_count = source.query("SELECT COUNT(*) AS row_count FROM `#{table}`")
                    .to_a.first["row_count"]
  puts "Converting table '#{table}' (#{row_count} rows)..."
  bar = ProgressBar.new(row_count)

  inserter = BulkInserter.new(target, table, columns)

  # Copy rows
  source.query("SELECT * FROM `#{table}`", stream: true, cache_rows: false).each do |row|
    values = columns.map do |name, opts|
      coerce_value(opts[:data_type], row[name])
    end

    inserter.insert(values)
    bar.increment! if row_count > 1000
  end
  inserter.flush!

  # Update id_seq
  next unless columns.keys.include?("id")
  max_id = target.exec("SELECT MAX(id) AS id FROM \"#{table}\"")
                 .first.values_at("id").first
  next unless max_id
  target.exec(
    "ALTER SEQUENCE \"#{table}_id_seq\" RESTART WITH " +
    (max_id.to_i + 1).to_s
  )
end

target.exec("COMMIT")
